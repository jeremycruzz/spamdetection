{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spam Detection NLP Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-24 21:53:30.295739: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-05-24 21:53:30.470194: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1748145210.535576   28141 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1748145210.555004   28141 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1748145210.695575   28141 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1748145210.695616   28141 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1748145210.695617   28141 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1748145210.695618   28141 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-24 21:53:30.713439: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "[nltk_data] Downloading package stopwords to /home/ujerm/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/ujerm/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/ujerm/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "/home/ujerm/.virtualenvs/mediapipe_env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, GlobalAveragePooling1D, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from utils.preprocessing import clean_text\n",
    "from utils.eda import plot_message_lengths, plot_top_words_by_class, plot_venn_words\n",
    "from utils.baseline import run_baseline_model\n",
    "from utils.bert import run_bert_model\n",
    "from utils.csv import write_stats_to_csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5572</td>\n",
       "      <td>5572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>2</td>\n",
       "      <td>5169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>ham</td>\n",
       "      <td>Sorry, I'll call later</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>4825</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       label                 message\n",
       "count   5572                    5572\n",
       "unique     2                    5169\n",
       "top      ham  Sorry, I'll call later\n",
       "freq    4825                      30"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://raw.githubusercontent.com/justmarkham/pycon-2016-tutorial/master/data/sms.tsv\"\n",
    "df = pd.read_csv(url, sep='\\t', names=[\"label\", \"message\"])\n",
    "\n",
    "df.head()\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove Stop-Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Examples where all variants are different:\n",
      "\n",
      "Example 1 (ham):\n",
      "Original: Nah I don't think he goes to usf, he lives around here though\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'normalized'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 33\u001b[39m\n\u001b[32m     31\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mExample \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexample[\u001b[33m'\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m):\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     32\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mOriginal: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexample[\u001b[33m'\u001b[39m\u001b[33mmessage\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNormalized:      \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mexample\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mnormalized\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     34\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mStop:     \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexample[\u001b[33m'\u001b[39m\u001b[33mstop\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     35\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLemma:    \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexample[\u001b[33m'\u001b[39m\u001b[33mstop_lemma\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyError\u001b[39m: 'normalized'"
     ]
    }
   ],
   "source": [
    "VARIANTS = ['raw', 'normalized', 'stop', 'stop_lemma', 'stop_stem']\n",
    "df['raw'] = df['message']\n",
    "df['normalized'] = df['message'].apply(lambda x: clean_text(x, 'normalized'))\n",
    "df['stop'] = df['message'].apply(lambda x: clean_text(x, 'stop'))\n",
    "df['stop_lemma'] = df['message'].apply(lambda x: clean_text(x, 'stop_lemma'))\n",
    "df['stop_stem'] = df['message'].apply(lambda x: clean_text(x, 'stop_stem'))\n",
    "\n",
    "\n",
    "# Print examples where all variants are different\n",
    "print(\"\\nExamples where all variants are different:\")\n",
    "different_variants = []\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    variants = [row[variant] for variant in VARIANTS]\n",
    "    # Check if all variants are different from each other\n",
    "    if len(set(variants)) == len(VARIANTS):\n",
    "        different_variants.append({\n",
    "            'message': row['message'],\n",
    "            'normalized': row['normalized'],\n",
    "            'stop': row['stop'],\n",
    "            'stop_lemma': row['stop_lemma'],\n",
    "            'stop_stem': row['stop_stem'],\n",
    "            'label': row['label']\n",
    "        })\n",
    "        # Limit to 5 examples to avoid overwhelming output\n",
    "        if len(different_variants) >= 5:\n",
    "            break\n",
    "\n",
    "# Display the examples in a formatted way\n",
    "for i, example in enumerate(different_variants, 1):\n",
    "    print(f\"\\nExample {i} ({example['label']}):\")\n",
    "    print(f\"Original: {example['message']}\")\n",
    "    print(f\"Normalized:      {example['normalized']}\")\n",
    "    print(f\"Stop:     {example['stop']}\")\n",
    "    print(f\"Lemma:    {example['stop_lemma']}\")\n",
    "    print(f\"Stem:     {example['stop_stem']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for variant in VARIANTS:\n",
    "    plot_message_lengths(df, column=variant, label_column='label', title=variant, save_path=f'data/eda/{variant}/message_lengths.png')\n",
    "    plot_top_words_by_class(df, column=variant, label_value='spam', top_n=20, title=f'Top 20 words in Spam Messages ({variant})',save_path=f'data/eda/{variant}/top_spam.png')\n",
    "    plot_top_words_by_class(df, column=variant, label_value='ham', top_n=20, title=f'Top 20 words in Ham Messages ({variant})', save_path=f'data/eda/{variant}/top_ham.png')\n",
    "    plot_venn_words(df, column=variant, title=f'Venn Diagram of Unique Words in Messages ({variant})', save_path=f'data/eda/{variant}/venn.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Evaluate Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 202505"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for variant in VARIANTS:\n",
    "    print(variant)\n",
    "    _, _, baseline_stats = run_baseline_model(df, column=variant, label_column='label', test_size=0.2, random_state=RANDOM_STATE)\n",
    "    write_stats_to_csv(baseline_stats, f'data/stats.csv')\n",
    "\n",
    "print('ham_as_spam')\n",
    "for missed in baseline_stats['misclassified']['ham_as_spam'][:10]:\n",
    "    print(missed)\n",
    "\n",
    "print('spam_as_ham')\n",
    "for missed in baseline_stats['misclassified']['spam_as_ham'][:10]:\n",
    "    print(missed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for variant in VARIANTS:\n",
    "    print(variant)\n",
    "    _, _, bert_stats = run_bert_model(df, column=variant, label_column='label', test_size=0.2, random_state=RANDOM_STATE)\n",
    "    write_stats_to_csv(bert_stats, f'data/stats.csv')\n",
    "\n",
    "    print('ham_as_spam')\n",
    "    for missed in bert_stats['misclassified']['ham_as_spam'][:10]:\n",
    "        print(missed)\n",
    "\n",
    "    print('spam_as_ham')\n",
    "    for missed in bert_stats['misclassified']['spam_as_ham'][:10]:\n",
    "        print(missed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mediapipe_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
